{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of a single layer neural network.\n",
    "\n",
    "Each training example corresponds to one neural and hence this example is of 4 neurons in one layer.\n",
    "\n",
    "Activation function - Sigmoid function\n",
    "\n",
    "Backpropogation - take derivative of sigmoid function, gradient descent\n",
    "\n",
    "| Sr.no | Input | Output |\n",
    "|-------|-------|--------|\n",
    "|Example1| 1,0,1 | 1 |\n",
    "|Example2 | 0,1,0 | 0 |\n",
    "|Example3 | 0,0,1 | 1 |\n",
    "|Example4 | 1,0,0 | 0 |\n",
    "|Example5 | 0,1,1 | ? (expected 1) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define input\n",
    "# [[Example1],[Example2],[Example3],[Example4]]\n",
    "# this gives 1x4 array. 1 element is one training example i.e., [1,0,1]\n",
    "x = np.array([[1,0,1],[0,1,0],[0,0,1],[1,0,0]]) \n",
    "\n",
    "# Define output\n",
    "# [[Example1],\n",
    "#  [Example2],\n",
    "#  [Example3],\n",
    "#  [Example4]]\n",
    "# this gives a column array i.e., 4x1 array.\n",
    "y = np.array([[1,0,1,0]]).T #.T is transpose. Converts row array to column array\n",
    "\n",
    "# Define weights\n",
    "# [[weight1],\n",
    "#  [weight2],\n",
    "#  [weight3]]\n",
    "weights = np.random.random((3,1)) \n",
    "# this gives 3x1 column array. Keeping the weight between 0 and 1 for normalized results.\n",
    "# we are taking three weights because there are 3 input columns for each training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to try several iterations to find the minimum error. \n",
    "# Have taken constant 500 for the example.\n",
    "\n",
    "for iteration in range(500):\n",
    "    \n",
    "    # take the dot product between training example and weight\n",
    "    z = np.dot(x, weights)\n",
    "    \n",
    "    # activation sigmoid function\n",
    "    sigmoid = 1/(1+np.exp(-z))\n",
    "    \n",
    "    # ----forward propagation ends here ------\n",
    "    \n",
    "    # find the error, that the difference between the output of the neuron and the actual output\n",
    "    # we have to minimize this error i.e. minimize the cost function\n",
    "    error = y - sigmoid\n",
    "    \n",
    "    # ----backpropagation starts here --------\n",
    "    \n",
    "    # Now, we learn the weights. How do we do that? Through gradient descent.\n",
    "    # We take the derivative of the sigmoid function since we have taken sigmoid function as\n",
    "    # our activation function.\n",
    "    \n",
    "    # sigmoidDerivative is the adjustment required in the weights of each training example.\n",
    "    sigmoidDerivative = sigmoid * (1-sigmoid)\n",
    "    \n",
    "    # We then take the above adjustment and recaluclate the weights accordingly.\n",
    "    # We multiply the adjustment by error, because we want to make sure that adjustment is able to consider\n",
    "    # even small changes as it nears to zero\n",
    "    \n",
    "    # we take dot product with the input, because weight of each dimension in training examples is different, but\n",
    "    # same for the same dimension in every example. i.e. column 1 of every training example the weight is\n",
    "    # same, but different than column 2 and column 3 of every training example. We need to calculate which \n",
    "    # column is important for every training example and hence to include the importance of each column we \n",
    "    # take dot product with the input\n",
    "    weights += np.dot(x.T, error*sigmoidDerivative)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New data : [0,1,1]\n",
      "[0.91500054]\n"
     ]
    }
   ],
   "source": [
    "# Now that we have trained the model, lets try it on new data\n",
    "\n",
    "print('New data : [0,1,1]')\n",
    "\n",
    "newz = np.dot(np.array([0,1,1]), weights)\n",
    "\n",
    "confidence = 1/(1+np.exp(-newz))\n",
    "\n",
    "print(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
